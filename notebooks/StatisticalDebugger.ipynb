{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Statistical Debugging\n",
    "\n",
    "In this chapter, we introduce _statistical debugging_ – the idea that specific events during execution could be _statistically correlated_ with failures. We start with coverage of individual lines and then proceed towards further execution features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bookutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "<!-- Automatically generated. Do not edit. -->\n",
    "\n",
    "\n",
    "\n",
    "_For those only interested in using the code in this chapter (without wanting to know how it works), give an example.  This will be copied to the beginning of the chapter (before the first section) as text with rendered input and output._\n",
    "\n",
    "You can use `int_fuzzer()` as:\n",
    "\n",
    "```python\n",
    "print(int_fuzzer())\n",
    "```\n",
    "```python\n",
    "=> 76.5\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The idea behind _statistical debugging_ is fairly simple. We have a program that sometimes passes and sometimes fails. This outcome can be _correlated_ with events that precede it – properties of the input, properties of the execution, properties of the program state. If we, for instance, can find that \"the program always fails when Line 123 is executed, and it always passes when Line 123 is _not_ executed\", then we have a strong correlation between Line 123 being executed and failure.\n",
    "\n",
    "Such _correlation_ does not necessarily mean _causation_. For this, we would have to prove that executing Line 123 _always_ leads to failure, and that _not_ executing it does not lead to (this) failure. Also, a correlation (or even a causation) does not mean that Line 123 contains the defect – for this, we would have to show that it actually is an error. Still, correlations make excellent hints as it comes to search for failure causes – in all generality, if you let your search be guided by _events that correlate with failures_, you are more likely to find _important hints on how the failure comes to be_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Collecting Events\n",
    "\n",
    "How can we determine events that correlate with failure? We start with a general mechanism to actually _collect_ events during execution. The abstract `Collector` class provides\n",
    "\n",
    "* a `collect()` method made for collecting events, called from the `traceit()` tracer; and\n",
    "* an `events()` method made for retrieving these events.\n",
    "\n",
    "Both of these are _abstract_ and will be defined further in subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tracer import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector(Tracer):\n",
    "    \"\"\"A class to record events during execution.\"\"\"\n",
    "\n",
    "    def collect(self, frame, event, arg):\n",
    "        \"\"\"Collecting function. To be overridden in subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def events(self):\n",
    "        \"\"\"Return a collection of events. To be overridden in subclasses.\"\"\"\n",
    "        return set()\n",
    "\n",
    "    def traceit(self, frame, event, arg):\n",
    "        self.collect(frame, event, arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Collector` class is used like `Tracer`, using a `with` statement. Let us apply it on the buggy variant of `remove_html_markup()` from the [Introduction to Debugging](Intro_Debugging.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_markup(s):\n",
    "    tag = False\n",
    "    quote = False\n",
    "    out = \"\"\n",
    "\n",
    "    for c in s:\n",
    "        if c == '<' and not quote:\n",
    "            tag = True\n",
    "        elif c == '>' and not quote:\n",
    "            tag = False\n",
    "        elif c == '\"' or c == \"'\" and tag:\n",
    "            quote = not quote\n",
    "        elif not tag:\n",
    "            out = out + c\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Collector()\n",
    "with c:\n",
    "    out = remove_html_markup('\"foo\"')\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's not much we can do with our collector, as the `collect()` and `events()` methods are yet empty. However, we can introduce an `id()` method which returns a string identifying the collector. This string is defined from the _first function call_ encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector(Collector):\n",
    "    def __init__(self):\n",
    "        self._id = None\n",
    "\n",
    "    def traceit(self, frame, event, arg):\n",
    "        if self._id is None and event == 'call':\n",
    "            # Save ID\n",
    "            function = frame.f_code.co_name\n",
    "            locals = frame.f_locals\n",
    "            args = \", \".join([f\"{var}={repr(locals[var])}\" for var in locals])\n",
    "            self._id = f\"{function}({args})\"\n",
    "\n",
    "        self.collect(frame, event, arg)\n",
    "\n",
    "    def id(self):\n",
    "        return self._id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Collector()\n",
    "with c:\n",
    "    remove_html_markup('abc')\n",
    "c.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Coverage\n",
    "\n",
    "So far, our `Collector` class does not collect any events. Let us extend it such that it collects _coverage_ information – that is, the set of lines executed. To this end, we introduce a `CoverageCollector` subclass which saves the coverage in a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageCollector(Collector):\n",
    "    \"\"\"A class to record covered lines during execution.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.coverage = set()\n",
    "\n",
    "    def collect(self, frame, event, arg):\n",
    "        self.coverage.add(frame.f_lineno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also override `events()` such that it returns the set of covered lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageCollector(CoverageCollector):\n",
    "    def events(self):\n",
    "        \"\"\"Return a set of predicates holding for the execution\"\"\"\n",
    "        return self.coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can use `CoverageCollector` to determine the lines executed during a run of `remove_html_markup()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CoverageCollector()\n",
    "with c:\n",
    "    remove_html_markup('abc')\n",
    "print(c.events())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets of line numbers alone are not too revealing. They provide more insights if we actually list the code, highlighting these numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import getsourcelines    # like inspect.getsourcelines(), but in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_with_coverage(function, coverage):\n",
    "    source_lines, starting_line_number = \\\n",
    "       getsourcelines(function)\n",
    "\n",
    "    line_number = starting_line_number\n",
    "    for line in source_lines:\n",
    "        marker = '*' if line_number in coverage else ' '\n",
    "        print(f\"{line_number:4} {marker} {line}\", end='')\n",
    "        line_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_coverage(remove_html_markup, c.coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the input `s` was `\"abc\"`? In this listing, we can see which lines were covered and which lines were not. From the listing already, we can see that `s` has neither tags nor quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such coverage computation plays a big role in _testing_, as one wants tests to cover as many different aspects of program execution (and notably code) as possible. But also during debugging, code coverage is essential: If some code was not even executed in the failing run, then any change to it will have no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz(\"Let the input be <code>&quot;&lt;b&gt;Don't do this!&lt;/b&gt;&quot;</code>. \"\n",
    "     \"Which of these lines are executed? Use the code to find out!\",\n",
    "     [\n",
    "         \"<code>tag = True</code>\",\n",
    "         \"<code>tag = False</code>\",\n",
    "         \"<code>quote = not quote</code>\",\n",
    "         \"<code>out = out + c</code>\"\n",
    "     ], [ord(c) - ord('a') - 1 for c in 'cdf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the solution, try this out yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CoverageCollector()\n",
    "with c:\n",
    "    remove_html_markup(\"<b>Don't do this!</b>\")\n",
    "# list_with_coverage(remove_html_markup, c.coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Differences\n",
    "\n",
    "Let us get back to the idea that we want to _correlate_ events with passing and failing outcomes. For this, we need to examine events in both _passing_ and _failing_ runs, and determine their _differences_ – since it is these differences we want to associate with their respective outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Base Class for Statistical Debugging\n",
    "\n",
    "The `StatisticalDebugger` base class takes a collector class (such as `CoverageCollector`). Its `collect()` method creates a new collector of that very class, which will be maintained by the debugger. As argument, `collect()` takes a string characterizing the outcome (such as `'PASS'` or `'FAIL'`). This is how one would use it:\n",
    "\n",
    "```python\n",
    "debugger = StatisticalDebugger(CoverageCollector)\n",
    "with debugger.collect('PASS'):\n",
    "    some_passing_run()\n",
    "with debugger.collect('PASS'):\n",
    "    another_passing_run()\n",
    "with debugger.collect('FAIL'):\n",
    "    some_failing_run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement `StatisticalDebugger`. The base class gets a collector class as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger():\n",
    "    \"\"\"A class to collect events for multiple outcomes.\"\"\"\n",
    "\n",
    "    def __init__(self, collector_class):\n",
    "        self.collector_class = collector_class\n",
    "        self.collectors = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collect()` method creates (and stores) a collector for the given outcome, using the given outcome to characterize the run. Any additional arguments are passed to the collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger(StatisticalDebugger):\n",
    "    def collect(self, outcome, *args):\n",
    "        collector = self.collector_class(*args)\n",
    "        if outcome not in self.collectors:\n",
    "            self.collectors[outcome] = []\n",
    "        self.collectors[outcome].append(collector)\n",
    "        return collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `all_events()` method produces a union of all events observed. If an outcome is given, it produces a union of all events with that outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger(StatisticalDebugger):\n",
    "    def all_events(self, outcome=None):\n",
    "        all_events = set()\n",
    "        if outcome:\n",
    "            for collector in self.collectors[outcome]:\n",
    "                all_events.update(collector.events())\n",
    "        else:\n",
    "            for outcome in self.collectors:\n",
    "                for collector in self.collectors[outcome]:\n",
    "                    all_events.update(collector.events())\n",
    "        return all_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example of `StatisticalDebugger` in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StatisticalDebugger(CoverageCollector)\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup(\"abc\")\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup('<b>abc</b>')\n",
    "with s.collect('FAIL'):\n",
    "    remove_html_markup('\"foo\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `all_events()` returns all events collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.all_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If given an outcome as argument, we obtain all events with the given outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.all_events('FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `collectors` maps outcomes to lists of collectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.collectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the collector of the one (and first) passing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.collectors['PASS'][0].id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.collectors['PASS'][0].events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better highlight the differences between the collected events, we introduce a method `event_table()` that prints out whether an event took place in a run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excursion: Printing an Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger(StatisticalDebugger):\n",
    "    def event_table(self, show_ids=False):\n",
    "        sep = ' | '\n",
    "\n",
    "        all_events = self.all_events()\n",
    "        longest_event = max(len(f\"{event}\") for event in all_events)\n",
    "\n",
    "        out = \"\"\n",
    "\n",
    "        # Header\n",
    "        if show_ids:\n",
    "            out += '| ' + ' ' * longest_event + sep\n",
    "            for name in self.collectors:\n",
    "                for collector in self.collectors[name]:\n",
    "                    out += '`' + collector.id() + '`' + sep\n",
    "            out += '\\n'\n",
    "        else:\n",
    "            out += '| ' + ' ' * longest_event + sep\n",
    "            for name in self.collectors:\n",
    "                for i in range(len(self.collectors[name])):\n",
    "                    out += name + sep\n",
    "            out += '\\n'\n",
    "\n",
    "        out += '| ' + '-' * longest_event + sep\n",
    "        for name in self.collectors:\n",
    "            for i in range(len(self.collectors[name])):\n",
    "                out += '-' * len(name) + sep\n",
    "        out += '\\n'\n",
    "\n",
    "        # Data\n",
    "        for event in all_events:\n",
    "            out += f\"| {repr(event).rjust(longest_event)}\" + sep\n",
    "            for name in self.collectors:\n",
    "                for collector in self.collectors[name]:\n",
    "                    out += ' ' * (len(name) - 1)\n",
    "                    if event in collector.events():\n",
    "                        out += \"X\"\n",
    "                    else:\n",
    "                        out += \"-\"\n",
    "                    out += sep\n",
    "            out += '\\n'\n",
    "\n",
    "        return Markdown(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StatisticalDebugger(CoverageCollector)\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup(\"abc\")\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup('<b>abc</b>')\n",
    "with s.collect('FAIL'):\n",
    "    remove_html_markup('\"foo\"')\n",
    "s.event_table(show_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz(\"How many lines are executed in the failing run only?\",\n",
    "    [\"One\", \"Two\", \"Three\"], int(chr(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines only executed in the failing run would be a correlation to look for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Passing and Failing Runs\n",
    "\n",
    "While our `StatisticalDebugger` class allows arbitrary outcomes, we are typically only interested in two outcomes, namely _passing_ vs. _failing_ runs. We therefore introduce a specialized `DifferenceDebugger()` class that provides customized methods to collect and access passing and failing runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceDebugger(StatisticalDebugger):\n",
    "    \"\"\"A class to collect events for passing and failing outcomes.\"\"\"\n",
    "\n",
    "    PASS = 'PASS'\n",
    "    FAIL = 'FAIL'\n",
    "\n",
    "    def collect_pass(self):\n",
    "        return self.collect(self.PASS)\n",
    "    def collect_fail(self):\n",
    "        return self.collect(self.FAIL)\n",
    "\n",
    "    def pass_collectors(self):\n",
    "        return self.collectors[self.PASS]\n",
    "    def fail_collectors(self):\n",
    "        return self.collectors[self.FAIL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to use `DifferenceDebugger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_debugger_html(debugger):\n",
    "    with debugger.collect_pass():\n",
    "        remove_html_markup('abc')\n",
    "    with debugger.collect_pass():\n",
    "        remove_html_markup('<b>abc</b>')\n",
    "    with debugger.collect_fail():\n",
    "        remove_html_markup('\"foo\"')\n",
    "    return debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DifferenceDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since events come back as _sets_, we can compute _unions_ and _differences_ between these sets. For instance, we can compute which lines were executed in _any_ of the passing runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_1_events = debugger.pass_collectors()[0].events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_2_events = debugger.pass_collectors()[1].events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_any_pass = pass_1_events | pass_2_events\n",
    "in_any_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can determine which lines were _only_ executed in the failing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_events = debugger.fail_collectors()[0].events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_in_fail = fail_events - in_any_pass\n",
    "only_in_fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the \"failing\" run is characterized by processing quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_coverage(remove_html_markup, only_in_fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add a few helper methods that return computations such as the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceDebugger(DifferenceDebugger):\n",
    "    def all_fail(self):\n",
    "        return self.all_events(self.FAIL)\n",
    "\n",
    "    def all_pass(self):\n",
    "        return self.all_events(self.PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now introduce helper methods that show the events occurring only in passing and failing runs, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceDebugger(DifferenceDebugger):\n",
    "    def only_fail(self):\n",
    "        return self.all_fail() - self.all_pass()\n",
    "\n",
    "    def only_pass(self):\n",
    "        return self.all_pass() - self.all_fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DifferenceDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.all_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the lines executed only in the failing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_fail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the lines executed only in the passing runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, having these lines individually is neat, but things become much more interesting if we can see the associated code lines just as well. That's what we will do in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Differences\n",
    "\n",
    "To show correlations of line coverage in context, we introduce a number of _visualization_ techniques that _highlight_ code with different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Spectrum\n",
    "\n",
    "The first idea is to use a _discrete_ spectrum of three colors:\n",
    "\n",
    "* _red_ for code executed in failing runs only\n",
    "* _green_ for code executed in passing runs only\n",
    "* _yellow_ for code executed in both passing and failing runs.\n",
    "\n",
    "Code that is not executed stays unhighlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `DiscreteSpectrumDebugger` subclass provides a `color()` method that returns one of these three colors depending on the line number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSpectrumDebugger(DifferenceDebugger):\n",
    "    def color(self, line_number):\n",
    "        passing = self.all_pass()\n",
    "        failing = self.all_fail()\n",
    "\n",
    "        if line_number in passing and line_number in failing:\n",
    "            return 'lightyellow'\n",
    "        elif line_number in failing:\n",
    "            return 'mistyrose'\n",
    "        elif line_number in passing:\n",
    "            return 'honeydew'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `list_with_spectrum()` method takes a function and shows each of its source code lines using the given spectrum, using HTML markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSpectrumDebugger(DiscreteSpectrumDebugger):\n",
    "    def list_with_spectrum(self, function, show_color_names=False):\n",
    "        source_lines, starting_line_number = \\\n",
    "           inspect.getsourcelines(function)\n",
    "\n",
    "        line_number = starting_line_number\n",
    "        out = \"\"\n",
    "        for line in source_lines:\n",
    "            if line.strip() == '':\n",
    "                line = '&nbsp;'\n",
    "\n",
    "            line = str(line_number).rjust(4) + ' ' + line\n",
    "            color = self.color(line_number)\n",
    "\n",
    "            if show_color_names:\n",
    "                line = f'{repr(color):20} {line}'\n",
    "\n",
    "            if color:\n",
    "                line = f'<pre style=\"background-color:{color}\">' \\\n",
    "                        f'{line.rstrip()}</pre>'\n",
    "            else:\n",
    "                line = f'<pre>{line}</pre>'\n",
    "\n",
    "            out += line\n",
    "            line_number += 1\n",
    "\n",
    "        return HTML(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the `only_pass()` and `only_fail()` sets look like when visualized with code. The \"culprit\" line is well highlighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DiscreteSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum(remove_html_markup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the failure is correlated with the presence of quotes in the input string (which is an important hint!). But does this also show us _immediately_ where the defect to be fixed is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz(\"Does the line <code>quote = not quote</code> actually contain the defect?\",\n",
    "    [\n",
    "        \"Yes, it should be fixed\",\n",
    "        \"No, the defect is elsewhere\"\n",
    "    ],\n",
    "     164 * 2 % 326\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is the preceding condition that is wrong. In order to fix a program, we have to find a location that\n",
    "\n",
    "1. _causes_ the failure (i.e., it can be changed to make the failure go away); and\n",
    "2. is a _defect_ (i.e., contains an error).\n",
    "\n",
    "In our example above, the highlighted code line is a _symptom_ for the error. To some extent, it is also a _cause_, since, say, commenting it out would also resolve the given failure, at the cost of causing other failures. However, the preceding condition also is a cause, as is the presence of quotes in the input.\n",
    "\n",
    "Only one of these also is a _defect_, though, and that is the preceding condition. Hence, while correlations can provide important hints, they do not necessarily locate defects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Spectrum\n",
    "\n",
    "The criterion that an event should _only_ occur in failing runs (and not in passing runs) can be too aggressive. In particular, if we have another run that executes the \"culprit\" lines, but does _not_ fail, our \"only in fail\" criterion will no longer be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example. The input\n",
    "\n",
    "```html\n",
    "<b color=\"blue\">text</b>\n",
    "```\n",
    "\n",
    "will trigger the \"culprit\" line\n",
    "\n",
    "```python\n",
    "quote = not quote\n",
    "```\n",
    "\n",
    "but actually produce an output where the tags are properly stripped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_html_markup('<b color=\"blue\">text</b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, we no longer have lines that are being executed only in failing runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DiscreteSpectrumDebugger(CoverageCollector))\n",
    "with debugger.collect_pass():\n",
    "    remove_html_markup('<b link=\"blue\"></b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_fail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our spectrum output, the effect now is that the \"culprit\" line is as yellow as all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum(remove_html_markup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore introduce a different method for highlighting lines, based on their _relative_ occurrence with respect to all runs: If a line has been _mostly_ executed in failing runs, its color should shift towards red; if a line has been _mostly_ executed in passing runs, its color should shift towards green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This _continuous spectrum_ has been introduced by the seminal _Tarantula_ tool. In Tarantula, the color _hue_ for each line is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textit{color hue}(\\textit{line}) = \\textit{low color(red)} + \\frac{\\%\\textit{passed}(\\textit{line})}{\\%\\textit{passed}(\\textit{line}) + \\%\\textit{failed}(\\textit{line})} \\times \\textit{color range}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `%passed` and `%failed` denote the percentage at which a line has been executed in passing and failing runs, respectively. A hue of 0.0 stands for red, a hue of 1.0 stands for green, and a hue of 0.5 stands for equal fractions of red and green, yielding yellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement these measures right away as methods in a new `ContinuousSpectrumDebugger` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousSpectrumDebugger(DiscreteSpectrumDebugger):\n",
    "    def event_fraction(self, event, category):\n",
    "        all_runs = self.collectors[category]\n",
    "        runs_with_event = set(collector for collector in all_runs \n",
    "                              if event in collector.events())\n",
    "        fraction = len(runs_with_event) / len(all_runs)\n",
    "        # print(f\"%{category}({event}) = {fraction}\")\n",
    "        return fraction\n",
    "\n",
    "    def passed(self, line_number):\n",
    "        return self.event_fraction(line_number, self.PASS)\n",
    "\n",
    "    def failed(self, line_number):\n",
    "        return self.event_fraction(line_number, self.FAIL)\n",
    "\n",
    "    def hue(self, line_number):\n",
    "        passed = self.passed(line_number)\n",
    "        failed = self.failed(line_number)\n",
    "        if passed + failed > 0:\n",
    "            return passed / (passed + failed)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hue for lines executed only in failing runs is (deep) red, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ContinuousSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_fail():\n",
    "    print(line, debugger.hue(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the hue for lines executed in passing runs is (deep) green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_pass():\n",
    "    print(line, debugger.hue(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tarantula tool not only sets the hue for a line, but also uses _brightness_ as measure for support – that is, how often was the line executed at all. The brighter a line, the stronger the correlation with a passing or failing outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brightness is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textit{bright}(line) = \\max(\\%\\textit{passed}(\\textit{line}), \\%\\textit{failed}(\\textit{line}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it is easily implemented, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousSpectrumDebugger(ContinuousSpectrumDebugger):\n",
    "    def bright(self, line):\n",
    "        return max(self.passed(line), self.failed(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our single \"only in fail\" line has a brightness of 1.0 (the maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ContinuousSpectrumDebugger(CoverageCollector))\n",
    "for line in debugger.only_fail():\n",
    "    print(line, debugger.bright(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can now define a color for each line. To this end, we override the (previously discrete) `color()` method such that it returns a color specification giving hue and brightness. We use the HTML format `hsl(hue, saturation, lightness)` where the hue is given as a value between 0 and 360 (0 is red, 120 is green) and saturation and lightness are provided as percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousSpectrumDebugger(ContinuousSpectrumDebugger):\n",
    "    def color(self, line):\n",
    "        hue = debugger.hue(line)\n",
    "        if hue is None:\n",
    "            return None\n",
    "        saturation = debugger.bright(line)\n",
    "\n",
    "        # HSL color values are specified with: \n",
    "        # hsl(hue, saturation, lightness).\n",
    "        return f\"hsl({hue * 120}, {saturation * 100}%, 80%)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ContinuousSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines executed only in failing runs are still shown in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_fail():\n",
    "    print(line, debugger.color(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... whereas lines executed only in passing runs are still shown in green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_pass():\n",
    "    print(line, debugger.color(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum(remove_html_markup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with our `quote = not quote` \"culprit\" line if it is executed in passing runs, too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with debugger.collect_pass():\n",
    "    out = remove_html_markup('<b link=\"blue\"></b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz('In which color will the <code>quote = not quote</code> \"culprit\" line be shown after executing the above code?',\n",
    "    [\n",
    "        '<span style=\"background-color: hsl(120.0, 50.0%, 80%)\">Green</span>',\n",
    "        '<span style=\"background-color: hsl(60.0, 100.0%, 80%)\">Yellow</span>',\n",
    "        '<span style=\"background-color: hsl(30.0, 100.0%, 80%)\">Orange</span>',\n",
    "        '<span style=\"background-color: hsl(0.0, 100.0%, 80%)\">Red</span>'\n",
    "    ],\n",
    "     999 / 333\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it still is shown with an orange-red tint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum(remove_html_markup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example, coming right from the Tarantula paper. The `middle()` function takes three numbers `x`, `y`, and `z`, and returns the one that is neither the minimum nor the maximum of the three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle(x, y, z):\n",
    "    if y < z:\n",
    "        if x < y:\n",
    "            return y\n",
    "        elif x < z:\n",
    "            return y\n",
    "    else:\n",
    "        if x > y:\n",
    "            return y\n",
    "        elif x > z:\n",
    "            return x\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle(1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `middle()` can fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle(2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let is see whether we can find the bug with a few additional test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_debugger_middle(debugger):\n",
    "    with debugger.collect_pass():\n",
    "        middle(3, 3, 5)\n",
    "    with debugger.collect_pass():\n",
    "        middle(1, 2, 3)\n",
    "    with debugger.collect_pass():\n",
    "        middle(3, 2, 1)\n",
    "    with debugger.collect_pass():\n",
    "        middle(5, 5, 5)\n",
    "    with debugger.collect_pass():\n",
    "        middle(5, 3, 4)\n",
    "    with debugger.collect_fail():\n",
    "        middle(2, 1, 3)\n",
    "    return debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to collect data from multiple function invocations, you need to have a separate `with` clause for every invocation. The following will _not_ work correctly:\n",
    "\n",
    "```python\n",
    "    with debugger.collect_pass():\n",
    "        middle(3, 3, 5)\n",
    "        middle(1, 2, 3)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ContinuousSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.event_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here us the visualization. We see that the `return y` line is the culprit here – and actually also the one to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum(middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz(\"Which of the above lines should be fixed?\",\n",
    "    [\n",
    "        '<span style=\"background-color: hsl(45.0, 100%, 80%)\">Line 3: <code>elif x &lt; y</code></span>',\n",
    "        '<span style=\"background-color: hsl(34.28571428571429, 100.0%, 80%)\">Line 5: <code>elif x &lt; z</code></span>',\n",
    "        '<span style=\"background-color: hsl(20.000000000000004, 100.0%, 80%)\">Line 6: <code>return y</code></span>',\n",
    "        '<span style=\"background-color: hsl(120.0, 20.0%, 80%)\">Line 9: <code>return y</code></span>',\n",
    "    ],\n",
    "     len(\" middle \\n\".strip()[:3])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in the `middle()` example, the \"reddest\" line is also the one to be fixed.  Here is the fixed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_fixed(x, y, z):\n",
    "    if y < z:\n",
    "        if x < y:\n",
    "            return y\n",
    "        elif x < z:\n",
    "            return x\n",
    "    else:\n",
    "        if x > y:\n",
    "            return y\n",
    "        elif x > z:\n",
    "            return x\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_fixed(2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Lines by Suspiciousness\n",
    "\n",
    "In a large program, there can be several locations (and events) that could be flagged as suspicious. It suffices that some large code block of say, 1,000 lines, is mostly executed in failing runs, and then all of this code block will be visualized in some shade of red. \n",
    "\n",
    "To further highlight the \"most suspicious\" events, one idea is to use a _ranking_ – that is, coming up with a list of events where those events most correlated with failures would be shown at the top. The programmer would then examine these events one by one and proceed down the list. We will show how this works for two \"correlation\" metrics – first the _Tarantula_ metric, as introduced above, and then the _Ochiai_ metric, which has shown to be one of the best \"ranking\" metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a base class `RankingDebugger` with an abstract method `suspiciousness()` to be overloaded in subclasses. The method `rank_by_suspiciousness()` returns a list of all events observed, sorted by suspiciousness, highest first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDebugger(DifferenceDebugger):\n",
    "    def suspiciousness(self, event):\n",
    "        return 0\n",
    "\n",
    "    def rank_by_suspiciousness(self):\n",
    "        events = list(self.all_events())\n",
    "        events.sort(key=lambda event: self.suspiciousness(event),\n",
    "                   reverse=True)\n",
    "        return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tarantula Metric\n",
    "\n",
    "We can use the Tarantula metric to sort lines according to their suspiciousness. The \"redder\" a line (a hue of 0.0), the more suspicious it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the `TarantulaDebugger` class, inheriting visualization capabilities from the `ContinuousSpectrumDebugger` class as well as the suspiciousness features from the `RankingDebugger` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarantulaDebugger(ContinuousSpectrumDebugger, RankingDebugger):\n",
    "    def suspiciousness(self, event):\n",
    "        return 1 - self.hue(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us list `remove_html_markup()` with highlighted lines again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(TarantulaRankingDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum(remove_html_markup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our ranking of lines, from most suspicious to least suspicious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.rank_by_suspiciousness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first line in the list is indeed the most suspicious; the two \"green\" lines come at the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ochiai Metric\n",
    "\n",
    "The _Ochiai_ Metric, as introduced in \\cite{...} ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Effective is Ranking?\n",
    "\n",
    "Such a ranking has shown to be very useful for _evaluations_, ...\n",
    "It has also shown to be useful for _automated repair_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Events besides Coverage\n",
    "\n",
    "Our framework allows for tracking arbitrary events, not just coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueCollector(Collector):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vars = set()\n",
    "\n",
    "    def collect(self, frame, event, arg):\n",
    "        local_vars = frame.f_locals\n",
    "        for var in local_vars:\n",
    "            value = local_vars[var]\n",
    "            self.vars.add((var, value))\n",
    "\n",
    "    def events(self):\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DifferenceDebugger(ValueCollector))\n",
    "debugger.event_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(DifferenceDebugger(ValueCollector))\n",
    "debugger.event_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_fail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(DifferenceDebugger):\n",
    "    PASS_VALUE = +1\n",
    "    FAIL_VALUE = -1\n",
    "\n",
    "    def samples(self):\n",
    "        samples = {}\n",
    "        for collector in self.pass_collectors():\n",
    "            samples[collector.id()] = self.PASS_VALUE\n",
    "        for collector in debugger.fail_collectors():\n",
    "            samples[collector.id()] = self.FAIL_VALUE\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def features(self):\n",
    "        features = {}\n",
    "        for collector in debugger.pass_collectors():\n",
    "            features[collector.id()] = collector.events()\n",
    "        for collector in debugger.fail_collectors():\n",
    "            features[collector.id()] = collector.events()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def feature_names(self):\n",
    "        return [repr(feature) for feature in self.all_events()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def shape(self, sample):\n",
    "        x = []\n",
    "        features = self.features()\n",
    "        for f in self.all_events():\n",
    "            if f in features[sample]:\n",
    "                x += [+1]\n",
    "            else:\n",
    "                x += [-1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.shape('middle(z=5, y=3, x=3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def X(self):\n",
    "        X = []\n",
    "        samples = self.samples()\n",
    "        for key in samples:\n",
    "            X += [self.shape(key)]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def Y(self):\n",
    "        Y = []\n",
    "        samples = self.samples()\n",
    "        for key in samples:\n",
    "            Y += [samples[key]]\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.Y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def classifier(self):\n",
    "        classifier = tree.DecisionTreeClassifier()\n",
    "        classifier = classifier.fit(self.X(), self.Y())\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def show_classifier(self, classifier):\n",
    "        dot_data = tree.export_graphviz(classifier, out_file=None, \n",
    "                         filled=False, rounded=True,\n",
    "                         feature_names=self.feature_names(),\n",
    "                                class_names=[\"fail\", \"pass\"],\n",
    "                                impurity=False,\n",
    "                         special_characters=True)\n",
    "        dot_data = dot_data.replace('&le; 0.0', ': no')\n",
    "        dot_data = dot_data.replace('&ge; 0.0', ': yes')\n",
    "\n",
    "        return graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tree we get.  A decision like `* <= 0` means that `*` is not part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "classifier = debugger.classifier()\n",
    "debugger.show_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def predict(self, classifier, sample):\n",
    "        return classifier.predict([self.shape(sample)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ClassifyingDebugger(CoverageCollector))\n",
    "# debugger.predict(classifier, set(166))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For those only interested in using the code in this chapter (without wanting to know how it works), give an example.  This will be copied to the beginning of the chapter (before the first section) as text with rendered input and output._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this is what we get for `x=1`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `int_fuzzer()` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2 + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Lesson one_\n",
    "* _Lesson two_\n",
    "* _Lesson three_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _mutations_ on existing inputs to get more valid inputs](MutationFuzzer.ipynb)\n",
    "* [use _grammars_ (i.e., a specification of the input format) to get even more valid inputs](Grammars.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](Reducer.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "_Cite relevant works in the literature and put them into context, as in:_\n",
    "\n",
    "The idea of ensuring that each expansion in the grammar is used at least once goes back to Burkhardt \\cite{Burkhardt1967}, to be later rediscovered by Paul Purdom \\cite{Purdom1972}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "_Close the chapter with a few exercises such that people have things to do.  To make the solutions hidden (to be revealed by the user), have them start with_\n",
    "\n",
    "```\n",
    "**Solution.**\n",
    "```\n",
    "\n",
    "_Your solution can then extend up to the next title (i.e., any markdown cell starting with `#`)._\n",
    "\n",
    "_Running `make metadata` will automatically add metadata to the cells such that the cells will be hidden by default, and can be uncovered by the user.  The button will be introduced above the solution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Solution for the exercise_"
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

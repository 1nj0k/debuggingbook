{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Semantic Debugging\n",
    "\n",
    "Given the many executions we can generate, it is only natural that these executions would also be subject to _machine learning_ in order to learn which features of the input (or the execution) would be associated with failures.\n",
    "\n",
    "In this chapter, we study the _Alhazen_ approach, one of the first of this kind.\n",
    "Alhazen by Kampmann et al. [[KHSZ20](https://publications.cispa.saarland/3107/7/fse2020-alhazen.pdf)] automatically learn the associations between the failure of a program and _features of the input data_, say \"The error occurs whenever the `<expr>` element is negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This chapter is based on an Alhazen implementation and exercise contributed by [Martin Eberlein](https://martineberlein.github.io), TU Berlin. Thanks a lot, Martin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bookutils import YouTubeVideo\n",
    "# YouTubeVideo(\"w4u5gCgPlmg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* This chapter extends the ideas from [the chapter on Generalizing Failure Circumstances](DDSetDebugger.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bookutils.setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "<!-- Automatically generated. Do not edit. -->\n",
    "\n",
    "\n",
    "\n",
    "_For those only interested in using the code in this chapter (without wanting to know how it works), give an example.  This will be copied to the beginning of the chapter (before the first section) as text with rendered input and output._\n",
    "\n",
    "You can use `int_fuzzer()` as:\n",
    "\n",
    "```python\n",
    "print(int_fuzzer())\n",
    "```\n",
    "```python\n",
    "=> 76.5\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Alhazen in a Nutshell\n",
    "\n",
    "when diagnosing why a program fails, the first step is to determine the circumstances under which the program failed. Kampmann et al. [[KHSZ20](https://publications.cispa.saarland/3107/7/fse2020-alhazen.pdf)] presented an approach to automatically discover the circumstances of program behavior.\n",
    "Their approach associates the program’s failure with the syntactical features of the input data, allowing them to learn and extract the properties that result in the specific behavior.\n",
    "\n",
    "Their reference implementation _Alhazen_ can generate a diagnosis and explain why, for instance, a particular bug occurs.\n",
    "More formally, Alhazen forms a hypothetical model based on the observed inputs.\n",
    "Additional test inputs are generated and executed to refine or refute the hypothesis, eventually obtaining a prediction model of the circumstances of why the behavior in question takes place.\n",
    "Alhazen use a _Decision Tree classifier_ to learn the association between the program behavior and the input features.\n",
    "\n",
    "![title](PICS/Alhazen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool is named after [Ḥasan Ibn al-Haytham](https://en.wikipedia.org/wiki/Ibn_al-Haytham) (latinized name: Alhazen).\n",
    "Often referred to as the \"Father of modern optics\", Ibn al-Haytham made significant contributions to the principles of optics and visual perception.\n",
    "Most notably, he was an early proponent of the concept that a hypothesis must be supported by experiments, and thus\n",
    "one of the inventors of the _scientific method_, the key process in the Alhazen tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.Grammars import Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALC_GRAMMAR: Grammar = {\n",
    "    \"<start>\":\n",
    "        [\"<function>(<term>)\"],\n",
    "\n",
    "    \"<function>\":\n",
    "        [\"sqrt\", \"tan\", \"cos\", \"sin\"],\n",
    "\n",
    "    \"<term>\": [\"-<value>\", \"<value>\"],\n",
    "\n",
    "    \"<value>\":\n",
    "        [\"<integer>.<integer>\",\n",
    "         \"<integer>\"],\n",
    "\n",
    "    \"<integer>\":\n",
    "        [\"<digit><integer>\", \"<digit>\"],\n",
    "\n",
    "    \"<digit>\":\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}\n",
    "START_SYMBOL = \"<start>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are concerned with the problem of extracting semantic features from inputs. In particular, Alhazen defines various features based on the input grammar, such as *existance* and *numeric interpretation*. These features are then extracted from the parse trees of the inputs (see [Section 3 of the paper](https://publications.cispa.saarland/3107/7/fse2020-alhazen.pdf) for more details).\n",
    "\n",
    "The implementation of the feature extraction module consists of the following three tasks:\n",
    "1. Implementation of individual feature classes, whose instances allow to derive specific feature values from inputs\n",
    "2. Extraction of features from the grammar through instantiation of the aforementioned feature classes\n",
    "3. Computation of feature vectors from a set of inputs, which will then be used as input for the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Feature(ABC):\n",
    "    '''\n",
    "    The abstract base class for grammar features.\n",
    "\n",
    "    Args:\n",
    "        name : A unique identifier name for this feature. Should not contain Whitespaces.\n",
    "               e.g., 'type(<feature>@1)'\n",
    "        rule : The production rule (e.g., '<function>' or '<value>').\n",
    "        key  : The feature key (e.g., the chosen alternative or rule itself).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, name: str, rule: str, key: str) -> None:\n",
    "        self.name = name\n",
    "        self.rule = rule\n",
    "        self.key = key\n",
    "        super().__init__()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        '''Returns a printable string representation of the feature.'''\n",
    "        return self.name_rep()\n",
    "\n",
    "    @abstractmethod\n",
    "    def name_rep(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_feature_value(self, derivation_tree) -> float:\n",
    "        '''Returns the feature value for a given derivation tree of an input.'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.GrammarFuzzer import expansion_to_children, DerivationTree\n",
    "\n",
    "class ExistenceFeature(Feature):\n",
    "    '''\n",
    "    This class represents existence features of a grammar. Existence features indicate\n",
    "    whether a particular production rule was used in the derivation sequence of an input.\n",
    "    For a given production rule P -> A | B, a production existence feature for P and\n",
    "    alternative existence features for each alternative (i.e., A and B) are defined.\n",
    "\n",
    "    name : A unique identifier name for this feature. Should not contain Whitespaces.\n",
    "           e.g., 'exist(<digit>@1)'\n",
    "    rule : The production rule.\n",
    "    key  : The feature key, equal to the rule attribute for production features,\n",
    "           or equal to the corresponding alternative for alternative features.\n",
    "    '''\n",
    "    def __init__(self, name: str, rule: str, key: str) -> None:\n",
    "        super().__init__(name, rule, key)\n",
    "\n",
    "    def name_rep(self) -> str:\n",
    "        if self.rule == self.key:\n",
    "            return f\"exists({self.rule})\"\n",
    "        else:\n",
    "            return f\"exists({self.rule} == {self.key})\"\n",
    "\n",
    "    def get_feature_value(self, derivation_tree) -> float:\n",
    "        '''Returns the feature value for a given derivation tree of an input.'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_feature_value(self, derivation_tree: DerivationTree) -> float:\n",
    "        '''Counts the number of times this feature was matched in the derivation tree.'''\n",
    "        (node, children) = derivation_tree\n",
    "\n",
    "        # The local match count (1 if the feature is matched for the current node, 0 if not)\n",
    "        count = 0\n",
    "\n",
    "        # First check if the current node can be matched with the rule\n",
    "        if node == self.rule:\n",
    "\n",
    "            # Production existance feature\n",
    "            if self.rule == self.key:\n",
    "                count = 1\n",
    "\n",
    "            # Production alternative existance feature\n",
    "            # We compare the children of the expansion with the actual children\n",
    "            else:\n",
    "                expansion_children = list(map(lambda x: x[0], expansion_to_children(self.key)))\n",
    "                node_children = list(map(lambda x: x[0], children))\n",
    "                if expansion_children == node_children:\n",
    "                    count= 1\n",
    "\n",
    "        # Recursively compute the counts for all children and return the sum for the whole tree\n",
    "        for child in children:\n",
    "            count = max(count, self.get_feature_value(child)) \n",
    "\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.GrammarFuzzer import tree_to_string\n",
    "from numpy import nanmax, isnan\n",
    "\n",
    "class NumericInterpretation(Feature):\n",
    "    '''\n",
    "    This class represents numeric interpretation features of a grammar. These features\n",
    "    are defined for productions that only derive words composed of the characters\n",
    "    [0-9], '.', and '-'. The returned feature value corresponds to the maximum\n",
    "    floating-point number interpretation of the derived words of a production.\n",
    "\n",
    "    name : A unique identifier name for this feature. Should not contain Whitespaces.\n",
    "           e.g., 'num(<integer>)'\n",
    "    rule : The production rule.\n",
    "    '''\n",
    "    def __init__(self, name: str, rule: str) -> None:\n",
    "        super().__init__(name, rule, rule)\n",
    "\n",
    "    def name_rep(self) -> str:\n",
    "        return f\"num({self.key})\"\n",
    "\n",
    "    def get_feature_value(self, derivation_tree) -> float:\n",
    "        '''Returns the feature value for a given derivation tree of an input.'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_feature_value(self, derivation_tree: DerivationTree) -> float:\n",
    "        '''Determines the maximum float of this feature in the derivation tree.'''\n",
    "        (node, children) = derivation_tree\n",
    "\n",
    "        value = float('nan')\n",
    "        if node == self.rule:\n",
    "            try:\n",
    "                #print(self.name, float(tree_to_string(derivation_tree)))\n",
    "                value = float(tree_to_string(derivation_tree))\n",
    "            except ValueError:\n",
    "                #print(self.name, float(tree_to_string(derivation_tree)), \"err\")\n",
    "                pass\n",
    "\n",
    "        # Return maximum value encountered in tree, ignoring all NaNs\n",
    "        tree_values = [value] + [self.get_feature_value(c) for c in children]\n",
    "        if all(isnan(tree_values)):\n",
    "            return value\n",
    "        else:\n",
    "            return nanmax(tree_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Feature Sets from Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_existence(grammar: Grammar) -> List[ExistenceFeature]:\n",
    "    '''\n",
    "        Extracts all existence features from the grammar and returns them as a list.\n",
    "        grammar : The input grammar.\n",
    "    '''\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for rule in grammar:\n",
    "        # add the rule\n",
    "        features.append(ExistenceFeature(f\"exists({rule})\", rule, rule))\n",
    "        # add all alternatives\n",
    "        for count, expansion in enumerate(grammar[rule]):\n",
    "            features.append(ExistenceFeature(f\"exists({rule}@{count})\", rule, expansion))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.Grammars import reachable_nonterminals\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Regex for non-terminal symbols in expansions\n",
    "RE_NONTERMINAL = re.compile(r'(<[^<> ]*>)')\n",
    "\n",
    "def extract_numeric(grammar: Grammar) -> List[NumericInterpretation]:\n",
    "    '''\n",
    "        Extracts all numeric interpretation features from the grammar and returns them as a list.\n",
    "\n",
    "        grammar : The input grammar.\n",
    "    '''\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Mapping from non-terminals to derivable terminal chars\n",
    "    derivable_chars = defaultdict(set)\n",
    "\n",
    "    for rule in grammar:\n",
    "        for expansion in grammar[rule]:\n",
    "            # Remove non-terminal symbols and whitespace from expansion\n",
    "            terminals = re.sub(RE_NONTERMINAL, '', expansion).replace(' ', '')\n",
    "\n",
    "            # Add each terminal char to the set of derivable chars\n",
    "            for c in terminals:\n",
    "                derivable_chars[rule].add(c)\n",
    "\n",
    "    # Repeatedly update the mapping until convergence\n",
    "    while True:\n",
    "        updated = False\n",
    "        for rule in grammar:\n",
    "            for r in reachable_nonterminals(grammar, rule):\n",
    "                before = len(derivable_chars[rule])\n",
    "                derivable_chars[rule].update(derivable_chars[r])\n",
    "                after = len(derivable_chars[rule])\n",
    "\n",
    "                # Set of derivable chars was updated\n",
    "                if after > before:\n",
    "                    updated = True\n",
    "\n",
    "        if not updated:\n",
    "            break\n",
    "\n",
    "    numeric_chars = set(['0','1','2','3','4','5','6','7','8','9','.','-'])\n",
    "\n",
    "    for key in derivable_chars:\n",
    "        # Check if derivable chars contain only numeric chars\n",
    "        if len(derivable_chars[key] - numeric_chars) == 0:\n",
    "            features.append(NumericInterpretation(f\"num({key})\", key))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(grammar: Grammar) -> List[Feature]:\n",
    "    return extract_existence(grammar) + extract_numeric(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_features(CALC_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Feature Values from Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This is a rather slow implementation, for many grammars with many syntactically features, the feature collection can be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.Parser import EarleyParser\n",
    "from fuzzingbook.Grammars import Grammar\n",
    "import pandas\n",
    "from pandas import DataFrame\n",
    "\n",
    "def collect_features(sample_list: List[str],\n",
    "                     grammar: Grammar) -> DataFrame:\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # parse grammar and extract features\n",
    "    all_features = get_all_features(grammar)\n",
    "\n",
    "    # iterate over all samples\n",
    "    for sample in sample_list:\n",
    "        parsed_features = {}\n",
    "        parsed_features[\"sample\"] = sample\n",
    "        # initate dictionary\n",
    "        for feature in all_features:\n",
    "            parsed_features[feature.name] = 0\n",
    "\n",
    "        # Obtain the parse tree for each input file\n",
    "        earley = EarleyParser(grammar)\n",
    "        for tree in earley.parse(sample):\n",
    "\n",
    "            for feature in all_features:\n",
    "                parsed_features[feature.name] = feature.get_feature_value(tree)\n",
    "\n",
    "        data.append(parsed_features)\n",
    "\n",
    "    return pandas.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [\"sqrt(-900)\", \"sin(24)\", \"cos(-3.14)\"]\n",
    "collect_features(sample_list, CALC_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle multiple trees\n",
    "from fuzzingbook.Parser import EarleyParser\n",
    "\n",
    "def compute_feature_values(sample: str, grammar: Grammar, features: List[Feature]) -> Dict[str, float]:\n",
    "    '''\n",
    "        Extracts all feature values from an input.\n",
    "\n",
    "        sample   : The input.\n",
    "        grammar  : The input grammar.\n",
    "        features : The list of input features extracted from the grammar.\n",
    "\n",
    "    '''\n",
    "    earley = EarleyParser(CALC_GRAMMAR)\n",
    "\n",
    "    features = {}\n",
    "    for tree in earley.parse(sample):\n",
    "        for feature in get_all_features(CALC_GRAMMAR):\n",
    "            features[feature.name_rep()] = feature.get_feature_value(tree)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = get_all_features(CALC_GRAMMAR)\n",
    "for sample in sample_list:\n",
    "    print(f\"Features of {sample}:\")\n",
    "    features = compute_feature_values(sample, CALC_GRAMMAR, all_features)\n",
    "    for feature, value in features.items():\n",
    "        print(f\"    {feature}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Alhazen Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas\n",
    "\n",
    "GENERATOR_TIMEOUT = 10 # timeout in seconds\n",
    "\n",
    "class Alhazen:\n",
    "    def __init__(self, initial_inputs: List[str],\n",
    "                 grammar: Grammar,\n",
    "                 max_iter: int = 10,\n",
    "                 generator_timeout: int = 10):\n",
    "\n",
    "        self._initial_inputs = initial_inputs\n",
    "        self._grammar = grammar\n",
    "        self._max_iter = max_iter\n",
    "        self._previous_samples = None\n",
    "        self._data = None\n",
    "        self._trees = []\n",
    "        self._generator_timeout = generator_timeout\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        self._previous_samples = self._initial_inputs\n",
    "\n",
    "        self._all_features = extract_existence(self._grammar) + extract_numeric(self._grammar)\n",
    "        self._feature_names = [f.name for f in self._all_features]\n",
    "\n",
    "    def run(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _add_new_data(self, exec_data, feature_data):\n",
    "        joined_data = exec_data.join(feature_data.drop(['sample'], axis=1))\n",
    "\n",
    "        # Only add valid data\n",
    "        new_data = joined_data[(joined_data['oracle'] != OracleResult.UNDEF)]\n",
    "        new_data = joined_data.drop(joined_data[joined_data.oracle.astype(str) == \"UNDEF\"].index)\n",
    "        if 0 != len(new_data):\n",
    "            if self._data is None:\n",
    "                self._data = new_data\n",
    "            else:\n",
    "                self._data = pandas.concat([self._data, new_data], sort=False)\n",
    "\n",
    "    def _finalize(self):\n",
    "        return self._trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alhazen(Alhazen):\n",
    "    def run(self):\n",
    "        for iteration in range(self._max_iter):\n",
    "            print(f\"Starting Iteration: \" + str(iteration))\n",
    "            self._loop(self._previous_samples)\n",
    "\n",
    "        return self._finalize()\n",
    "\n",
    "    def _loop(self, sample_list):\n",
    "        # obtain labels, execute samples (Initial Step, Activity 5)\n",
    "        exec_data = execute_samples(sample_list)\n",
    "\n",
    "        # collect features from the new samples (Activity 1)\n",
    "        feature_data = collect_features(sample_list, self._grammar)\n",
    "\n",
    "        # combine the new data with the already existing data\n",
    "        self._add_new_data(exec_data, feature_data)\n",
    "\n",
    "        # train a tree (Activity 2)\n",
    "        dec_tree = train_tree(self._data)\n",
    "        self._trees.append(dec_tree)\n",
    "\n",
    "        # extract new requirements from the tree (Activity 3)\n",
    "        new_input_specifications = get_all_input_specifications(dec_tree,\n",
    "                                                self._all_features,\n",
    "                                                self._feature_names,\n",
    "                                                self._data.drop(['oracle'], axis=1))\n",
    "\n",
    "        # generate new inputs according to the new input specifications\n",
    "        # (Activity 4)\n",
    "        new_samples = generate_samples(self._grammar, new_input_specifications, self._generator_timeout)\n",
    "        self._previous_samples = new_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excursion: All the Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text will only show up on demand (HTML) or not at all (PDF). This is useful for longer implementations, or repetitive, or specialized parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## _Section 3_\n",
    "\n",
    "\\todo{Add}\n",
    "\n",
    "_If you want to introduce code, it is helpful to state the most important functions, as in:_\n",
    "\n",
    "* `random.randrange(start, end)` - return a random number [`start`, `end`]\n",
    "* `range(start, end)` - create a list with integers from `start` to `end`.  Typically used in iterations.\n",
    "* `for elem in list: body` executes `body` in a loop with `elem` taking each value from `list`.\n",
    "* `for i in range(start, end): body` executes `body` in a loop with `i` from `start` to `end` - 1.\n",
    "* `chr(n)` - return a character with ASCII code `n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_fuzzer() -> float:\n",
    "    \"\"\"A simple function that returns a random float\"\"\"\n",
    "    return random.randrange(1, 100) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# More code\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Section 4_\n",
    "\n",
    "\\todo{Add}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For those only interested in using the code in this chapter (without wanting to know how it works), give an example.  This will be copied to the beginning of the chapter (before the first section) as text with rendered input and output._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `int_fuzzer()` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_fuzzer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Lesson one_\n",
    "* _Lesson two_\n",
    "* _Lesson three_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _assertions_ to check conditions at runtime](Assertions.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](DeltaDebugger.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "_Cite relevant works in the literature and put them into context, as in:_\n",
    "\n",
    "The idea of ensuring that each expansion in the grammar is used at least once goes back to Burkhardt \\cite{Burkhardt1967}, to be later rediscovered by Paul Purdom \\cite{Purdom1972}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "_Close the chapter with a few exercises such that people have things to do.  To make the solutions hidden (to be revealed by the user), have them start with_\n",
    "\n",
    "```\n",
    "**Solution.**\n",
    "```\n",
    "\n",
    "_Your solution can then extend up to the next title (i.e., any markdown cell starting with `#`)._\n",
    "\n",
    "_Running `make metadata` will automatically add metadata to the cells such that the cells will be hidden by default, and can be uncovered by the user.  The button will be introduced above the solution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Solution for the exercise_"
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

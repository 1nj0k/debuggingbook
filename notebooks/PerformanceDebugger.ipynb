{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Debugging Performance Issues\n",
    "\n",
    "Most chapters of this book deal with _functional_ issues – that is, issues related to the _functionality_ (or its absence) of the code in question. However, debugging can also involve _nonfunctional_ issues, however – performance, usability, reliability, and more. In this chapter, we give a short introduction on how to debug such nonfunctional issues, notably _performance_ issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import YouTubeVideo\n",
    "YouTubeVideo(\"w4u5gCgPlmg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* This chapter leverages visualization capabilities from [the chapter on statistical debugging](StatisticalDebugger.ipynb)\n",
    "* We also show how to debug nonfunctional issues using [delta debugging](DeltaDebugger.ipynb) and [statistical debugging](StatisticalDebugger.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bookutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import StatisticalDebugger\n",
    "import DeltaDebugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Synopsis\n",
    "<!-- Automatically generated. Do not edit. -->\n",
    "\n",
    "To [use the code provided in this chapter](Importing.ipynb), write\n",
    "\n",
    "```python\n",
    ">>> from debuggingbook.PerformanceDebugger import <identifier>\n",
    "```\n",
    "\n",
    "and then make use of the following features.\n",
    "\n",
    "\n",
    "_For those only interested in using the code in this chapter (without wanting to know how it works), give an example.  This will be copied to the beginning of the chapter (before the first section) as text with rendered input and output._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n",
    "The solution to debugging performance issues fits in two simple rules:\n",
    "\n",
    "1. _Measure_ performance\n",
    "2. _Break down_ how individual parts of your code contribute to performance.\n",
    "\n",
    "The first part, actually _measuring_ performance, is key here. Developers often take elaborated guesses on which aspects of their code impact performance, and think about all possible ways to optimize their code – and at the same time, making it harder to understand, harder to evolve, and harder to maintain. In most cases, such guesses are wrong. Instead, _measure_ performance of your program, _identify_ the very few parts that may need to get improved, and again _measure_ the impact of your changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all programming languages offer a way to measure performance and breaking it down to individual parts of the code – a means also known as *profiling*. Profiling works by measuring the execution time for each function (or even more fine-grained location) in your program. This can be achieved by\n",
    "\n",
    "1. _Measuring_ the current time at entry and exit of each function (or line), thus determining the time spent; or\n",
    "2. _Sampling_ the current function call stack at regular intervals, and thus assessing which functions are most active (= take the most time) during execution.\n",
    "\n",
    "Depending on your programming language, profiling is either done via _measuring_ (which requires instrumentation) or _sampling_ (which requires a parallel thread sampling the execution). Python programs use the _measuring_ method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Python Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate profiling in a simple example. The `ChangeCounter` class (which we will encounter in the [chapter on mining version histories](ChangeCounter.ipynb) reads in a version history from a git repository. Yet, it takes more than a minute to read in the debugging book change history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChangeCounter import ChangeCounter, debuggingbook_change_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as t:\n",
    "    change_counter = debuggingbook_change_counter(ChangeCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python `profile` and `cProfile` modules offer a simple way to identify the most time-consuming functions. They are invoked using the `run()` function, whose argument is the command to be profiled. The output reports, for each function encountered:\n",
    "\n",
    "* How often it was called (`ncalls`)\n",
    "* How much time was spent in the given function, _excluding_ time spent in calls to sub-functions (`tottime`)\n",
    "* The fraction of `tottime` / `ncalls` (first `percall`)\n",
    "* How much time was spent in the given function, _including_ time spent in calls to sub-functions (`cumtime`)\n",
    "* The fraction of `cumtime` / `percall` (second `percall`)\n",
    "\n",
    "Let us have a look at the profile we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('debuggingbook_change_counter(ChangeCounter)', sort='cumulative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that's an awful lot of functions, but we can quickly narrow things down. The `cumtime` column is sorted by largest values first. We see that the `debuggingbook_change_counter()` method at the top takes up all the time – but this is not surprising, since it it the method we called in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next places are more interesting: almost all time is spent in a single method, named `modifications()`. This method determines the difference between two versions, which is an expensive operation; this is also supported by the observation that half of the time is spent in a `diff()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This profile thus already gets us a hint on how to improve performance: Rather than computing the diff between versions for _every_ version, we could do so _on demand_ (and possibly cache results so we don't have to compute them twice). Alas, this (slow) functionality is part of a third-party module, so we cannot do this within the `ChangeCounter` class. But we could file a bug with the developers, suggesting a patch to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying a culprit is not always that easy. Notably, when the first set of obvious performance hogs is fixed, it becomes more and more difficult to squeeze out additional performance – and, as stated above, such optimization may be in conflict with readability and maintainability of your code. Here are some simple ways to improve performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Efficient algorithms**. For many tasks, the simplest algorithm is not always the best performing one. Consider alternatives that may be more efficient, and _measure_ whether they pay off.\n",
    "\n",
    "* **Efficient data types**. Remember that certain operations, such as looking up whether an element is contained, may take different amounts of time depending on the data structure. In Python, a query like `x in xs` takes (mostly) constant time if `xs` is a set, but linear time if `xs` is a list; these differences become significant as the size of `xs` grows.\n",
    "\n",
    "* **Efficient modules**. In Python, most frequently used modules (or at least parts of) are implemented in C, which is way more efficient than plain Python. Rely on existing modules whenever possible. Or implement your own, _after_ having measured that this may pay off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all things you can already use during programming – and also set up your code such that exchanging, say, one data type by another will still be possible later. This is best achieved by hiding implementation details (such as the used data types) behind an abstract interface used by your clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But beyond these points, remember Knuth's words:\n",
    "\n",
    "> Premature optimization is the root of all evil.\n",
    "\n",
    "which – repeat after us – means that you should always _first_ measure and _then_ optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Profilers Work\n",
    "\n",
    "Having discussed profilers from a _user_ perspective, let us now dive into how they are actually implemented. It turns out we can use most of our existing infrastructure to implement a simple profiler with only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program we will apply our profiler on is – surprise! – our ongoing example, `remove_html_markup()`. Our aim is to understand how much time is spent _in each line of the code_ (such that we have a new feature on top of Python `cProfile`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Intro_Debugging import remove_html_markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "from typing import Any, Optional, Type, Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "from bookutils import print_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_content(inspect.getsource(remove_html_markup), '.py',\n",
    "              start_line_number=238)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a class `PerformanceTracer` that tracks, for each line in the code:\n",
    "\n",
    "* how _often_ it was executed (`hits`), and\n",
    "* _how much time_ was spent during its execution (`time`).\n",
    "\n",
    "To this end, we make use of our `Timer` class, which measures time, and the `Tracer` class from [the chapter on tracing](Tracer.ipynb), which allows us to track every line of the program as it is being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tracer import Tracer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PerfomanceTracker`, the attributes `hits` and `time` are mappings indexed by unique locations – that is, pairs of function name and line number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location = Tuple[str, int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracer(Tracer):\n",
    "    \"\"\"Trace time and #hits for individual program lines\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "        super().__init__()\n",
    "        self.reset_timer()\n",
    "        self.hits: Dict[Location, int] = {}\n",
    "        self.time: Dict[Location, float] = {}\n",
    "\n",
    "    def reset_timer(self) -> None:\n",
    "        self.timer = Timer.Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As common in this book, we want to use `PerformanceTracer` in a `with`-block around the function call(s) to be tracked:\n",
    "\n",
    "```python\n",
    "with PerformanceTracer() as perf_tracer:\n",
    "    function(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When entering the `with` block (`__enter__()`), we reset all timers. Also, coming from the `__enter__()` method of the superclass `Tracer`, we enable tracing through the `traceit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import FrameType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracer(PerformanceTracer):\n",
    "    def __enter__(self) -> Any:\n",
    "        \"\"\"Enter a `with` block.\"\"\"\n",
    "        super().__enter__()\n",
    "        self.reset_timer()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `traceit()` method extracts the current location. It increases the corresponding `hits` value by 1, and adds the elapsed time to the corresponding `time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracer(PerformanceTracer):\n",
    "    def traceit(self, frame: FrameType, event: str, arg: Any) -> None:\n",
    "        \"\"\"Tracing function; called for every line.\"\"\"\n",
    "        t = self.timer.elapsed_time()\n",
    "        location = (frame.f_code.co_name, frame.f_lineno)\n",
    "\n",
    "        self.hits.setdefault(location, 0)\n",
    "        self.time.setdefault(location, 0.0)\n",
    "        self.hits[location] += 1\n",
    "        self.time[location] += t\n",
    "\n",
    "        self.reset_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it already. We can now determine where most time is spent in `remove_html_markup()`. We invoke it 10,000 times such that we can average over runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PerformanceTracer() as perf_tracer:\n",
    "    for i in range(10000):\n",
    "        s = remove_html_markup('<b>foo</b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the hits. For every line executed, we see how often it was executed. The most executed line is the `for` loop with 110,000 hits – once for each of the 10 characters in `<b>foo</b>`, once for the final check, and all of this 10,000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_tracer.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `time` attribute collects how much time was spent in each line. Within the loop, again, the `for` statement takes the most time. The other lines show some variability, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_tracer.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full profiler, these numbers would now be sorted and printed in a table, much like `cProfile` does. However, we will borrow some material from previous chapters and annotate our code accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Metric Visualization\n",
    "\n",
    "In the [chapter on statistical debugging](StatisticalDebugger.ipynb), we have encountered the `CoverageCollector` class, which collects line and function coverage during execution, using a `collect()` method that is invoked for every line. We will repurpose this class to collect arbitrary _metrics_ on the lines executed, notably time taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Time Spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from StatisticalDebugger import CoverageCollector, SpectrumDebugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MetricCollector` class is an abstract superclass that provides an interface to access a particular metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCollector(CoverageCollector):\n",
    "    \"\"\"Abstract superclass for collecting line-specific metrics\"\"\"\n",
    "\n",
    "    def metric(self, event: Any) -> Optional[float]:\n",
    "        \"\"\"Return a metric for an event, or none.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def all_metrics(self, func: str) -> List[float]:\n",
    "        \"\"\"Return all metric for a function `func`.\"\"\"\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these metrics, we can also compute sums and maxima for a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCollector(MetricCollector):\n",
    "    def total(self, func: str) -> float:\n",
    "        return sum(self.all_metrics(func))\n",
    "\n",
    "    def maximum(self, func: str) -> float:\n",
    "        return max(self.all_metrics(func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instantiate this superclass into `TimeCollector` – a subclass that measures time. This is modeled after our `PerformanceTracer` class, above; notably, the `time` attribute serves the same role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCollector(MetricCollector):\n",
    "    \"\"\"Collect time executed for each line\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        super().__init__()\n",
    "        self.reset_timer()\n",
    "        self.time: Dict[Location, float] = {}\n",
    "        self.add_items_to_ignore([Timer.Timer, Timer.clock])\n",
    "\n",
    "    def collect(self, frame: FrameType, event: str, arg: Any) -> None:\n",
    "        \"\"\"Invoked for every line executed. Accumulate time spent.\"\"\"\n",
    "        t = self.timer.elapsed_time()\n",
    "        super().collect(frame, event, arg)\n",
    "        location = (frame.f_code.co_name, frame.f_lineno)\n",
    "\n",
    "        self.time.setdefault(location, 0.0)\n",
    "        self.time[location] += t\n",
    "\n",
    "        self.reset_timer()\n",
    "\n",
    "    def reset_timer(self) -> None:\n",
    "        self.timer = Timer.Timer()\n",
    "\n",
    "    def __enter__(self) -> Any:\n",
    "        super().__enter__()\n",
    "        self.reset_timer()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metric()` and `all_metrics()` methods accumulate the metric (time taken) for an individual function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCollector(TimeCollector):\n",
    "    def metric(self, location: Any) -> Optional[float]:\n",
    "        if location in self.time:\n",
    "            return self.time[location]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def all_metrics(self, func: str) -> List[float]:\n",
    "        return [time\n",
    "                for (func_name, lineno), time in self.time.items()\n",
    "                if func_name == func]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to use `TimeCollector()` – again, in a `with` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TimeCollector() as collector:\n",
    "    for i in range(100):\n",
    "        s = remove_html_markup('<b>foo</b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `time` attribute holds the time spent in each line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location, time in collector.time.items():\n",
    "    print(location, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also create a total for an entire function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.total('remove_html_markup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Time Spent\n",
    "\n",
    "Let us now go and visualize these numbers. The idea is to assign each line a color whose saturation indicates the time spent in that line relative to the time spent in the function overall  – the higher the fraction, the darker the line. We create a `MetricDebugger` class built as a specialization of `SpectrumDebugger`, in which `suspiciousness()` and `color()` are repurposed to show these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricDebugger(SpectrumDebugger):\n",
    "    \"\"\"Visualize a metric\"\"\"\n",
    "\n",
    "    def metric(self, location: Location) -> float:\n",
    "        sum = 0.0\n",
    "        for outcome in self.collectors:\n",
    "            for collector in self.collectors[outcome]:\n",
    "                assert isinstance(collector, MetricCollector)\n",
    "                m = collector.metric(location)\n",
    "                if m is not None:\n",
    "                    sum += m  # type: ignore\n",
    "\n",
    "        return sum\n",
    "\n",
    "    def total(self, func_name: str) -> float:\n",
    "        total = 0.0\n",
    "        for outcome in self.collectors:\n",
    "            for collector in self.collectors[outcome]:\n",
    "                assert isinstance(collector, MetricCollector)\n",
    "                total += sum(collector.all_metrics(func_name))\n",
    "\n",
    "        return total\n",
    "\n",
    "    def maximum(self, func_name: str) -> float:\n",
    "        maximum = 0.0\n",
    "        for outcome in self.collectors:\n",
    "            for collector in self.collectors[outcome]:\n",
    "                assert isinstance(collector, MetricCollector)\n",
    "                maximum = max(maximum, \n",
    "                              max(collector.all_metrics(func_name)))\n",
    "\n",
    "        return maximum\n",
    "\n",
    "    def suspiciousness(self, location: Location) -> float:\n",
    "        func_name, _ = location\n",
    "        return self.metric(location) / self.total(func_name)\n",
    "\n",
    "    def color(self, location: Location) -> str:\n",
    "        func_name, _ = location\n",
    "        hue = 240  # blue\n",
    "        saturation = 100  # fully saturated\n",
    "        darkness = self.metric(location) / self.maximum(func_name)\n",
    "        lightness = 100 - darkness * 25\n",
    "        return f\"hsl({hue}, {saturation}%, {lightness}%)\"\n",
    "\n",
    "    def tooltip(self, location: Location) -> str:\n",
    "        return f\"{super().tooltip(location)} {self.metric(location)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now introduce `PerformanceDebugger` as a subclass of `MetricDebugger`, using an arbitrary `MetricCollector` (such as `TimeCollector`) to obtain the metric we want to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceDebugger(MetricDebugger):\n",
    "    \"\"\"Collect and visualize a metric\"\"\"\n",
    "\n",
    "    def __init__(self, collector_class: Type, log: bool = False):\n",
    "        assert issubclass(collector_class, MetricCollector)\n",
    "        super().__init__(collector_class, log=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `PerformanceDebugger`, we inherit all the capabilities of `SpectrumDebugger`, such as showing the (relative) percentage of time spent in a table. We see that the `for` condition and the following `assert` take most of the time, followed by the first condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PerformanceDebugger(TimeCollector) as debugger:\n",
    "    for i in range(100):\n",
    "        s = remove_html_markup('<b>foo</b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(debugger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also visualize these percentages, using shades of blue to indicate those lines most time spent in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Metrics\n",
    "\n",
    "Our framework is flexible enough to collect (and visualize) arbitrary metrics. This `HitCollector` class, for instance, collects how often a line is being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HitCollector(MetricCollector):\n",
    "    \"\"\"Collect how often a line is executed\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.hits: Dict[Location, int] = {}\n",
    "\n",
    "    def collect(self, frame: FrameType, event: str, arg: Any) -> None:\n",
    "        super().collect(frame, event, arg)\n",
    "        location = (frame.f_code.co_name, frame.f_lineno)\n",
    "\n",
    "        self.hits.setdefault(location, 0)\n",
    "        self.hits[location] += 1\n",
    "\n",
    "    def metric(self, location: Location) -> Optional[int]:\n",
    "        if location in self.hits:\n",
    "            return self.hits[location]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def all_metrics(self, func: str) -> List[float]:\n",
    "        return [hits\n",
    "                for (func_name, lineno), hits in self.hits.items()\n",
    "                if func_name == func]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plug in this class into `PerformanceDebugger` to obtain a distribution of lines executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PerformanceDebugger(HitCollector) as debugger:\n",
    "    for i in range(100):\n",
    "        s = remove_html_markup('<b>foo</b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, during this call to `remove_html_markup()`, there are 6,400 lines executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.total('remove_html_markup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can visualize the distribution as a table and using colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(debugger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with Delta Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For those only interested in using the code in this chapter (without wanting to know how it works), give an example.  This will be copied to the beginning of the chapter (before the first section) as text with rendered input and output._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Lesson one_\n",
    "* _Lesson two_\n",
    "* _Lesson three_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _assertions_ to check conditions at runtime](Assertions.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](DeltaDebugger.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "_Cite relevant works in the literature and put them into context, as in:_\n",
    "\n",
    "The idea of ensuring that each expansion in the grammar is used at least once goes back to Burkhardt \\cite{Burkhardt1967}, to be later rediscovered by Paul Purdom \\cite{Purdom1972}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "_Close the chapter with a few exercises such that people have things to do.  To make the solutions hidden (to be revealed by the user), have them start with_\n",
    "\n",
    "```\n",
    "**Solution.**\n",
    "```\n",
    "\n",
    "_Your solution can then extend up to the next title (i.e., any markdown cell starting with `#`)._\n",
    "\n",
    "_Running `make metadata` will automatically add metadata to the cells such that the cells will be hidden by default, and can be uncovered by the user.  The button will be introduced above the solution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Solution for the exercise_"
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
